# HDF5: h5py & PyTables
### 10 July 2017
- Tom Kooij 

## Part One

### Introduction

In HDF5, all nodes stem from a root "/". The nodes can either be Groups or Datasets (or Leaves in PyTables). Groups == directories and can container Datasets or other Groups. A Dataset is a container for data.

It is a self-documenting container. The data is self-documenting. Includes metadata, datasets, etc.

Two packages in Python support HDF5: `H5py` (very like `numpy` and low-level) and `PyTables` (higher-level API; built on `pandas`). 

Three types of objects you can store:

	1. Dataset (array)
	2. Groups (folders)
	3. Attributes (Names, Values)

### Using the Hierarchy

With PyTables

	with tables.open_file(filename, 'w') as f:
    	f.create_group('/', 'a_group')
    	f.set_node_attr('/a_group', 'attribute_name', 'A string')

With H5py

	with h5py.File(filename,'w') as f:
    	f.create_group('a_group')
    	f['a_group'].attrs['attribute_name'] = 'A string'
    !h5dump {filename}

Can even store classes as attributes.

### Datasets and Datatypes

Create dataset in H5py

    arr_to_store = np.arange(10, dtype=np.int8)
    f = h5py.File(FILENAME, "w")
	f.create_dataset(data=arr_to_store, name="mydata")
	# Or, using numpy like-syntex,
	f['/mydata2'] = arr_to_store 

	# Can read it back
	f['/mydata'][:]

To investiage contents:

	!h5ls {FILENAME}

Create dataset in PyTables

	import tables
	f2 = tables.open_file(FILENAME, "w")
	f2.create_array(f2.root, name="mydata", obj=arr_to_store)

	# To read it back,
	f2.root.mydata[:] 
	# or,
	f2.root.mydata.read()  

Compound datatypes in H5py:

	dtype = np.dtype([("myfield1", np.int32), ("myfield2", np.float64), ("myfield3", "S5")])
	table_to_store = np.fromiter(((i, i**2, "foo_%d"%i) for i in range(10)), dtype=dtype)

	f = h5py.File(FILENAME, "w")
    f['mydata'] = table_to_store
    f.close()

Compound datatypes in PyTables:

	f2 = tables.open_file(FILENAME, "w")
	table = f2.create_table(f2.root, name="mydata", description=table_to_store.dtype)
	table.append(table_to_store)  

PyTables files are larger than h5py files because PyTables are chunked.

The `cols` accessor in PyTables can read a column.

	t.col('momentum')[2:5]
	# or 
	t.cols.momentum[2:5]

## Part Two

### Chunking

In HDF5 data is stored in row major order. 

	Rows are fast, columns are slow. 

*Because there is one read operation per row.*

Chunking is data stored in predefined size. Used for large datasets.

The HDF5 library always reads a *whole* chunk.

Chunk always has the same rank as a dataset. 

To create chunked dataset:

	f.create_dataset("data", (size,), chunks=(chunksize,), dtype=np.int64)

By default the chunk size is 1024.

Small chunks can decrease performance because of read/write for each chunk.  Don't chunk smaller than disk block size (4k/8k)

The chunkcache is 1Mb (h5py) or 2Mb (pytables) by default. Be sure chunks fit in the chunkcache.

### Compression

CPUs are "starved": they are faster than memory. So usually they're just waiting around for data.

Compression blocks and shuffles memory.

Access compression in pytables using `tables.Filters` class.

	filters = tables.Filters(complevel=2, complib='zlib')
	f.create_table(..., filters=filters)

`complevel` ranges from 0 (no compression) to 9 (maximum compression)

There is no best compressor. You have to figure out what best fits your data.

## Part Three

### Queries

`table.iterrows()` returns an iterator that iterates of ALL rows, avoiding loading the table into memory.

`table.where()` is an iterator that performs an in-kernel query. It returns a row iterator that iterates over the selected rows. This is faster.

### Indexed Queries

*If columns are indexed, queries can be accelerated.*

## Part Four

### Integration with Pandas

`pandas.HDFStore` acts like a `dict` similar to h5py.

Pandas automatically adds indexes to columns.

We can select with columns to index in order to conserve space on-disk.

## Part Five

### Cache Size
You can change the cache size by 

	# create file access properties. H5Pcreate()
	propfaid = h5py.h5p.create(h5py.h5p.FILE_ACCESS)

	nslots = 521
	chunk_cache_mem_size = 2 * 1024 * 1024  # Mb
	w0 = 0.75  # Determines how fast discard chunk
	settings[1:] = (nslots, chunk_cache_mem_size, w0)
 
	propfaid.set_cache(*settings)

	# open the file
	mode = h5py.h5f.ACC_TRUNC
	f = h5py.File(h5py.h5f.create(b'newfile.h5', flags=mode, fapl=propfaid))

	f.close()

### Parallel HDF5

Threadsafety in HDF5: not very optimized but safe.

Can change threadsafety by editing the file `libhdf5.settings` in your environment.

The `mpi4py` package is useful to parallelizing HDF5 using h5py. 